{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, dirname, basename, splitext, abspath\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import platform\n",
    "from IPython.display import display\n",
    "from IPython.core.display import display, HTML\n",
    "import platform\n",
    "from util.loader import load_raw_data, convert_data\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "this_folder = dirname(__file__) if '__file__' in globals() else abspath('')\n",
    "root_folder = dirname(dirname(this_folder))\n",
    "def get_test_result_folder(testname = 'baseline'):\n",
    "    return join(root_folder, 'test', 'testresults', 'XPS-15-9560', testname, 'CUTEst')\n",
    "\n",
    "def color_negative_red(val):\n",
    "    color = 'red' if val < 0 else 'black'\n",
    "    return f'color: {color}'\n",
    "def color_negative_red_positive_green(val):\n",
    "    if val > 0:\n",
    "        return 'color: green'\n",
    "    elif val < 0:\n",
    "        return 'color: red'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = get_test_result_folder('panoc-lbfgs-11-4')\n",
    "new_folder = get_test_result_folder('panoc-2nd-lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-there",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_raw = load_raw_data(base_folder)\n",
    "new_raw = load_raw_data(new_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = convert_data(base_raw)\n",
    "new_df = convert_data(new_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-senator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "new_df.style.format('{:.2e}', subset=(new_df.dtypes == float)) \\\n",
    "            .format('{:.8e}', subset=['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_stats(df):\n",
    "    conv = df['status'].value_counts()['Converged']\n",
    "    tot = df['status'].count()\n",
    "    tot_time = df['time'].sum()\n",
    "    conv_time = df.where(df['status'] == 'Converged')['time'].sum()\n",
    "    print(f'Converged:      {conv}/{tot} = {100*conv/tot:.02f}%')\n",
    "    print(f'Total time:     {tot_time:.03f}s')\n",
    "    print(f'Converged time: {conv_time:.03f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline\\n---\\n')\n",
    "df_stats(base_df)\n",
    "print('\\n')\n",
    "print('New test\\n---\\n')\n",
    "df_stats(new_df)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(a, b, columns):\n",
    "#     res = pd.DataFrame()\n",
    "#     for i, df in enumerate(dfs):\n",
    "#         res[f'{column} {i}'] = df[column]\n",
    "#     return res\n",
    "    res = a[columns].join(b[columns], lsuffix=' 0', rsuffix=' 1')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-multimedia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmp = compare_results(base_df, new_df, ['status', 'f', 'ε', 'δ']) #  'inner iterations', 'outer iterations',\n",
    "cmp['f imprv'] = cmp['f 0'] - cmp['f 1']\n",
    "cmp['rel f imprv'] = 100 * cmp['f imprv'] / abs(cmp['f 0'])\n",
    "cmp.sort_values('rel f imprv') \\\n",
    "   .style.applymap(color_negative_red_positive_green, subset=['f imprv', 'rel f imprv']) \\\n",
    "         .format('{:.2e}', subset=(cmp.dtypes == float)) \\\n",
    "         .format('{:.8e}', subset=['f 0', 'f 1']) \\\n",
    "         .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), cmp.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-differential",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "worse_f = cmp[(cmp['f imprv'] < 0) & (cmp['status 1'] == 'Converged')]\n",
    "better_f = cmp[(cmp['f imprv'] > 0) & (cmp['status 1'] == 'Converged')]\n",
    "tol = 1e-5\n",
    "really_worse_f = worse_f[abs(worse_f['rel f imprv']) > 100 * tol]\n",
    "really_better_f = better_f[abs(better_f['rel f imprv']) > 100 * tol]\n",
    "print(f'{len(worse_f)} tests got worse results')\n",
    "print(f'{len(really_worse_f)} tests got significantly worse')\n",
    "print(f'{len(better_f)} tests got better results')\n",
    "print(f'{len(really_better_f)} tests got significantly better')\n",
    "\n",
    "print('\\nSignificantly worse tests:')\n",
    "really_worse_f.sort_values('rel f imprv') \\\n",
    "              .style.applymap(color_negative_red_positive_green, subset=['f imprv', 'rel f imprv']) \\\n",
    "                    .format('{:.2e}', subset=(really_worse_f.dtypes == float)) \\\n",
    "                    .format('{:.8e}', subset=['f 0', 'f 1']) \\\n",
    "                    .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), really_worse_f.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-uniform",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statusses = base_df[['status']].join(new_df[['status']], lsuffix=' 0', rsuffix=' 1')\n",
    "not_conv_to_conv = (statusses['status 0'] != 'Converged') & (statusses['status 1'] == 'Converged')\n",
    "conv_to_not_conv = (statusses['status 0'] == 'Converged') & (statusses['status 1'] != 'Converged')\n",
    "\n",
    "print(f'{len(base_df[not_conv_to_conv])} tests that didn\\'t converge before do converge after the change')\n",
    "print(f'{len(base_df[conv_to_not_conv])} tests that converged before no longer converge after the change')\n",
    "\n",
    "display(HTML(\"<hr>\"))\n",
    "\n",
    "print('The following tests went from not converging to converging')\n",
    "display(base_df[not_conv_to_conv].style.format('{:.2e}', subset=(base_df.dtypes == float)) \\\n",
    "                                       .format('{:.8e}', subset=['f']) \\\n",
    "                                       .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), base_df.columns))))\n",
    "display(new_df[not_conv_to_conv].style.format('{:.2e}', subset=(new_df.dtypes == float)) \\\n",
    "                                      .format('{:.8e}', subset=['f']) \\\n",
    "                                      .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), new_df.columns))))\n",
    "\n",
    "display(HTML(\"<hr>\"))\n",
    "\n",
    "print('The following tests went from converging to no longer converging')\n",
    "display(base_df[conv_to_not_conv].style.format('{:.2e}', subset=(base_df.dtypes == float)) \\\n",
    "                                       .format('{:.8e}', subset=['f']) \\\n",
    "                                       .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), base_df.columns))))\n",
    "display(new_df[conv_to_not_conv].style.format('{:.2e}', subset=(new_df.dtypes == float)) \\\n",
    "                                      .format('{:.8e}', subset=['f']) \\\n",
    "                                      .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), new_df.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-product",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "both_converged = (base_df['status'] == 'Converged') & (new_df['status'] == 'Converged')\n",
    "cmp = compare_results(base_df[both_converged], new_df[both_converged], ['status', 'time', 'f evaluations', 'grad_f evaluations'])\n",
    "cmp['time imprv'] = cmp['time 0'] - cmp['time 1']\n",
    "cmp['rel time imprv'] = 100 * cmp['time imprv'] / cmp['time 0']\n",
    "cmp['f eval imprv'] = cmp['f evaluations 0'] - cmp['f evaluations 1']\n",
    "cmp['rel f eval imprv'] = 100 * cmp['f eval imprv'] / cmp['f evaluations 0']\n",
    "cmp['grad_f eval imprv'] = cmp['grad_f evaluations 0'] - cmp['grad_f evaluations 1']\n",
    "cmp['rel grad_f eval imprv'] = 100 * cmp['grad_f eval imprv'] / cmp['grad_f evaluations 0']\n",
    "print(f\"Net time improvement:      {cmp['time imprv'].sum()}\")\n",
    "print(f\"Relative time improvement: {100 * cmp['time imprv'].sum() / cmp['time 0'].sum():.02f}%\")\n",
    "print(f\"Net f eval improvement:      {cmp['f eval imprv'].sum()}\")\n",
    "print(f\"Relative f eval improvement: {100 * cmp['f eval imprv'].sum() / cmp['f evaluations 0'].sum():.02f}%\")\n",
    "print(f\"Net grad_f eval improvement:      {cmp['grad_f eval imprv'].sum()}\")\n",
    "print(f\"Relative grad_f eval improvement: {100 * cmp['grad_f eval imprv'].sum() / cmp['grad_f evaluations 0'].sum():.02f}%\")\n",
    "print('positive is good, negative is bad')\n",
    "\n",
    "\n",
    "print(f\"Time:                  {base_df[both_converged]['time'].sum()}\\t{new_df[both_converged]['time'].sum()}\")\n",
    "print(f\"Objective evaluations: {base_df[both_converged]['f evaluations'].sum()}\\t{new_df[both_converged]['f evaluations'].sum()}\")\n",
    "print(f\"Gradient evaluations:  {base_df[both_converged]['grad_f evaluations'].sum()}\\t{new_df[both_converged]['grad_f evaluations'].sum()}\")\n",
    "print(f\"Outer iterations:      {base_df[both_converged]['outer iterations'].sum()}\\t{new_df[both_converged]['outer iterations'].sum()}\")\n",
    "print(f\"Inner iterations:      {base_df[both_converged]['inner iterations'].sum()}\\t{new_df[both_converged]['inner iterations'].sum()}\")\n",
    "print(f\"Linesearch failures:   {base_df[both_converged]['linesearch failures'].sum()}\\t{new_df[both_converged]['linesearch failures'].sum()}\")\n",
    "print(f\"L-BFGS failures:       {base_df[both_converged]['L-BFGS failures'].sum()}\\t{new_df[both_converged]['L-BFGS failures'].sum()}\")\n",
    "print(f\"L-BFGS rejected:       {base_df[both_converged]['L-BFGS rejected'].sum()}\\t{new_df[both_converged]['L-BFGS rejected'].sum()}\")\n",
    "\n",
    "cmp.sort_values('rel f eval imprv') \\\n",
    "   .style.applymap(color_negative_red_positive_green, subset=['time imprv', 'rel time imprv', 'f eval imprv', 'rel f eval imprv', 'grad_f eval imprv', 'rel grad_f eval imprv']) \\\n",
    "         .format('{:.2e}', subset=(cmp.dtypes == float)) \\\n",
    "         .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), cmp.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-abuse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmp = compare_results(base_df[both_converged], new_df[both_converged], ['status', 'linesearch failures', 'L-BFGS failures', 'L-BFGS rejected'])\n",
    "cmp['ls imprv'] = cmp['linesearch failures 0'] - cmp['linesearch failures 1']\n",
    "cmp['rel ls imprv'] = 100 * cmp['ls imprv'] / cmp['linesearch failures 0']\n",
    "cmp['lbfgs imprv'] = cmp['L-BFGS failures 0'] - cmp['L-BFGS failures 1']\n",
    "cmp['rel lbfgs imprv'] = 100 * cmp['lbfgs imprv'] / cmp['L-BFGS failures 0']\n",
    "cmp['lbfgs rej imprv'] = cmp['L-BFGS rejected 0'] - cmp['L-BFGS rejected 1']\n",
    "cmp['rel lbfgs rej imprv'] = 100 * cmp['lbfgs rej imprv'] / cmp['L-BFGS rejected 0']\n",
    "print(f\"Net linesearch failures improvement:    {cmp['ls imprv'].sum()}\")\n",
    "print(f\"Relative linesearch improvement:        {100. * cmp['ls imprv'].sum() / cmp['linesearch failures 0'].sum():.02f}%\")\n",
    "print(f\"Net L-BFGS failures improvement:        {cmp['lbfgs imprv'].sum()}\")\n",
    "print(f\"Relative L-BFGS failures improvement:   {100. * cmp['lbfgs imprv'].sum() / cmp['L-BFGS failures 0'].sum():.02f}%\")\n",
    "print(f\"Net L-BFGS rejections improvement:      {cmp['lbfgs rej imprv'].sum()}\")\n",
    "print(f\"Relative L-BFGS rejections improvement: {100. * cmp['lbfgs rej imprv'].sum() / cmp['L-BFGS rejected 0'].sum():.02f}%\")\n",
    "\n",
    "cmp.sort_values('rel ls imprv') \\\n",
    "   .style.applymap(color_negative_red_positive_green, subset=['ls imprv', 'rel ls imprv', 'lbfgs imprv', 'rel lbfgs imprv', 'lbfgs rej imprv', 'rel lbfgs rej imprv']) \\\n",
    "         .format('{:.2e}', subset=(cmp.dtypes == float)) \\\n",
    "         .format('{:.02f}%', subset=list(filter(lambda s: s.startswith('rel'), cmp.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df[base_df['ε'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df['ε'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\"' + '\"\\n\"'.join(new_df[new_df['box constr x'] > 0].index.values) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df.index == 'HS75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df[base_df.index == 'HS75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python392jvsc74a57bd05d4654af3199d5fc6ec5bea3457d8073abad6f85647580598dff8c259a6b449c",
   "display_name": "Python 3.9.2 64-bit ('py-venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}