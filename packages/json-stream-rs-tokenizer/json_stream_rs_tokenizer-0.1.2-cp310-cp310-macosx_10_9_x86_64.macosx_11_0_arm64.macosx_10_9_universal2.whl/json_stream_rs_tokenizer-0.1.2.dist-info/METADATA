Metadata-Version: 2.1
Name: json-stream-rs-tokenizer
Version: 0.1.2
Classifier: Programming Language :: Rust
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Programming Language :: Python :: Implementation :: PyPy
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: MIT License
Requires-Dist: json-stream
Requires-Dist: tqdm; extra == 'benchmark'
Provides-Extra: benchmark
License-File: LICENSE
Requires-Python: >=3.7
Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM

# json-stream-rs-tokenizer

[![CI](https://github.com/smheidrich/py-json-stream-rs-tokenizer/actions/workflows/CI.yml/badge.svg)](https://github.com/smheidrich/py-json-stream-rs-tokenizer/actions/workflows/CI.yml)
[![PyPI](https://img.shields.io/pypi/pyversions/json-stream-rs-tokenizer?style=plastic)](https://pypi.org/project/json-stream-rs-tokenizer/)
[![Tag](https://img.shields.io/github/v/tag/smheidrich/py-json-stream-rs-tokenizer?style=plastic)](https://github.com/smheidrich/py-json-stream-rs-tokenizer/tags)

A faster tokenizer for the [json-stream](https://github.com/daggaz/json-stream)
Python library.

It's actually just `json-stream`'s own tokenizer (itself adapted from the
[NAYA](https://github.com/danielyule/naya) project) ported to Rust almost
verbatim and made available as a Python module using
[PyO3](https://github.com/PyO3/pyo3).

On my machine, it **speeds up parsing by a factor of 4â€“10**, depending on the
nature of the data.

## Installation

```bash
pip install json-stream-rs-tokenizer
```

**Note** that in editable/develop installs, it will sometimes (?) compile the
Rust library in debug mode, which makes it run *slower* than the pure-Python
tokenizer. When in doubt, run installation commands with `--verbose` to see the
Rust compilation commands and verify that they used `--release`.

## Usage

Because `json-stream` currently has no mechanism to provide a custom tokenizer
(which I would prefer), this package provides its own wrappres around
json_stream's `load` and `visit` functions that monkeypatch it in before
running them:

```python
from io import StringIO
import json_stream_rs_tokenizer import load

# uses the Rust tokenizer to load JSON:
d = load(StringIO('{ "a": [1,2,3,4], "b": [5,6,7] }'))

for k, l in d.items():
  print(f"{k}: {' '.join(str(n) for n in l)}")
```

The patching is undone when the function returns.

Due to patching being a global state mutation, using `json-stream-rs-tokenizer`
in this way is generally *not thread-safe*. As an alternative, you can patch it
in manually using `json_stream_rs_tokenizer.patch()`, which should be safe if
you do it before you spawn any threads, and then just call the original (but
now patched) `json_stream.load` and `json_stream.visit` functions.

## License

MIT license. Refer to the
[LICENSE](https://github.com/smheidrich/py-json-stream-rs-tokenizer/blob/main/LICENSE)
file for details.

