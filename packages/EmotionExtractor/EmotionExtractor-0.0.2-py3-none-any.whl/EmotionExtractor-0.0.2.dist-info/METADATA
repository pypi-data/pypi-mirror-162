Metadata-Version: 2.1
Name: EmotionExtractor
Version: 0.0.2
Summary: Extract Emotion Words from Text or Word Tokens
Home-page: https://github.com/riyadhctg/EmotionExtractor.git
Author: Md Riyadh
Author-email: mdriyadh@cmail.carleton.ca
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: nltk
Requires-Dist: setuptools
Requires-Dist: wheel

# EmotionExtractor
Extract emotion words from sentence or list of tokens.

## Installation
```
pip install EmotionExtractor
```

## Usage
```python
from emotionextractor.emotionextractor import EmotionExtractor

ee = EmotionExtractor()

sentence = "I am happy to see you succeed"
tokens = ["I", "am", "happy", "to", "see", "you", "succeed"]

ee.extract_emotion(sentence)
#or 
ee.extract_emotion(tokens)

#output
# ['happy', 'succeed']
```

`extract_emotion(...)` can take several other optional parameters in addition to input sentence/word tokens:

```
:param bool lemmatize: Set to True to enable lemmatization. default is False
:param bool clean_stopwords: Set to False to disable stop words removal. default is True
:param bool strict_mode: Set to True to enable strict choice of emotion words based on adj and adv. default is True
:param bool remove_pos: Set to True if you'd like to only allow certain Parts of speech (POS). default s false
:param list allowed_pos: List of POS you want to allow from nltk TAGSET: https://github.com/nltk/nltk/blob/develop/nltk/app/chunkparser_app.py
a more readable list from third party: https://www.guru99.com/pos-tagging-chunking-nltk.html When it is not set, and remove_pos is set to True, then by default this POS whitelist is used: ["RB", "RBS", "RBR", "JJ", "JJR", "JJS"]
:param str filter: It can be set to either 'N' or 'P'. default is None.
```


## Troubleshooting
If you recieve error regarding `nltk version not found` try:

```bash
pip install --upgrade nltk
```


## Additional Note
More info will be shared soon about the lexicon used 
